{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis - Análisis de Sentimiento.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RFajardoMonzon/MachineLearningCourse/blob/master/Sentiment_Analysis_An%C3%A1lisis_de_Sentimiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdPOYy098qfM",
        "colab_type": "text"
      },
      "source": [
        "# Notebook 15 - Análisis de sentimiento.\n",
        "\n",
        "\n",
        "*   Recuerda que puedes consultar la documentación sobre una función escribiendo **?** justo después de la función: *Ejemplo: np.maximum?*\n",
        "*   Puedes ejecutar el contenido de una celda con el atajo de teclado **CTRL+ENTER**\n",
        "*   Utiliza **TAB** cada vez que quieras autocompletar una llamada a una función.\n",
        "*   Puedes ejecutar instrucciones de bash directamente desde el notebook usando **!** : *Ejemplo: !pip install tensorflow*\n",
        "*   Recuerda que Google es tu amigo, y saber buscar la información en las documentaciones de las librerías es muy importante.\n",
        "*   Una solución correcta no es la que funciona sino la que se entiende!\n",
        "*   No dudes en preguntar cualquier duda al profesor que lleva todo el día dando la turra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm9U8EKw9Gb4",
        "colab_type": "text"
      },
      "source": [
        "## 1. ¿Te ha gustado esta película?\n",
        "\n",
        "\n",
        "Hoy vamos a trabajar con reviews de películas recopilas de la web IMDB. El objetivo será el de analizar el texto que describe la review de un usuario, y al mismo tiempo ver qué puntuación este le ha dado a la película. Buscaremos entrenar a un modelo que sepa entender el sentimiento asociado al texto escrito por un usuario *¿Es una descripción positiva, negativa, neutra?*\n",
        "\n",
        "> > > > > <img src=https://www.samyzaf.com/ML/imdb/review1.png width=500px>\n",
        "\n",
        "1. Comencemos analizando un poco nuestro *dataset* viendo cuáles son las palabras que con mayor frecuencia se repiten. Para ello, elige 1000 reviews de manera aleatoria, y contabiliza la frecuencia de sus palabras. Después, de ese listado, imprime cuáles son las **50 palabras de mayor frecuencia**. ¿Tienen sentido?\n",
        "2. Ahora ya podemos crear nuestro modelo para resolver este problema de clasificación. Has diferentes pruebas con diferentes tipos de arquitectura (i.e. con capas *LSTM()*, tipo multicapa, con *Embedders()*) y comprueba el rendimiento medido en accuracy y tiempo de entrenamiento de cada una de ellas. Busca una respuesta intuitiva de tus experimentos. Cuando tengas tu modelo entrenado, haz pruebas con texto tuyos propios.\n",
        "3. **(Bonus) :** Ahora puedes probar a usar capas de Embeddings ya pre-entrenados. Utiliza Vec2Word o GloVe en tu arquitectura y comprueba qué mejoras te aporta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M33d_-c3UUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "old = np.load\n",
        "\n",
        "np.load = lambda *a,**k: old(*a, allow_pickle=True, **k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiLZY2NZ24IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_features = 100 # Número máximo de palabras diferentes de nuestro vocabulario.\n",
        "maxlen = 300         # Número máximo de palabras en cada texto.\n",
        "\n",
        "# Carga el texto ya tokenizado.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features) \n",
        "\n",
        "# Preprocesado de los textos para igualar todas sus longitudes.\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) \n",
        "\n",
        "# Utiliza esta línea para obtener la palabra asociada a cada índice.\n",
        "# imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwXEOG3Q0aRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9MeyW2V1jCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = [key for key, _ in word_index.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3QZT4c41uVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys_dict = {v: k for k, v in word_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L12FTgst7Nv6",
        "colab_type": "code",
        "outputId": "aa424f3a-feff-4fb2-c482-ffb9c63e3bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(keys))\n",
        "print(len(keys_dict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88584\n",
            "88584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGkRvnoM8Nxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_indexes(array, size):\n",
        "  res = []\n",
        "  freqs = []\n",
        "  for _ in np.arange(size):\n",
        "    max_arg = np.argmax(array)\n",
        "    freqs.append(array[max_arg])\n",
        "    array[max_arg] = -1\n",
        "    res.append(max_arg)\n",
        "  return np.array(res), np.array(freqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8q1tKrk3IFM",
        "colab_type": "code",
        "outputId": "f6c403c5-d35d-4b97-e980-7094425e1ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "random_reviews = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]\n",
        "bincounts = np.array([np.bincount(review, minlength=len(keys_dict)+1) for review in random_reviews])\n",
        "\n",
        "# print(bincounts.shape)\n",
        "\n",
        "total_bincounts = np.sum(bincounts, axis=0)\n",
        "max_indexes, freqs = get_max_indexes(total_bincounts[1:], 50)\n",
        "print(max_indexes)\n",
        "most_frequent = np.array([keys_dict[word+1] for word in max_indexes])\n",
        "print([[w, v] for w, v in zip(most_frequent, freqs)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  3  4  5  6  7  8  9 10 12 11 13 14 15 16 19 18 17 20 21 22 23 24 25\n",
            " 27 29 26 28 30 31 32  0 33 34 35 37 36 38 39 40 45 41 42 44 47 49 43 50\n",
            " 46 48]\n",
            "[['and', 11009], ['of', 10985], ['to', 5366], ['is', 5216], ['br', 4637], ['in', 4345], ['it', 3531], ['i', 3490], ['this', 3106], ['was', 2678], ['that', 2610], ['as', 2531], ['for', 2192], ['with', 1602], ['movie', 1517], ['on', 1516], ['film', 1447], ['but', 1440], ['not', 1400], ['you', 1208], ['are', 1155], ['his', 1021], ['have', 1007], ['he', 994], ['one', 925], ['at', 900], ['be', 888], ['all', 872], ['by', 868], ['an', 819], ['they', 753], ['the', 742], ['who', 730], ['so', 726], ['from', 703], ['her', 700], ['like', 669], ['or', 661], ['just', 659], ['about', 607], ['some', 593], [\"it's\", 587], ['out', 580], ['if', 580], ['what', 575], ['more', 544], ['has', 536], ['when', 527], ['there', 520], ['good', 513]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYBH7XbWDBfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nModel = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Amz6YbV8-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-MeOhBmBP4p",
        "colab_type": "code",
        "outputId": "c8eeaa5a-ba04-47ee-8db1-d59edb21c332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, 128))\n",
        "\n",
        "model.add(CuDNNLSTM(128))\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=SGD(1.5), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=8, validation_data=(x_test, y_test))\n",
        "\n",
        "model.save(\"LSTM{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/8\n",
            "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.6815 - acc: 0.5462 - val_loss: 0.7055 - val_acc: 0.5272\n",
            "Epoch 2/8\n",
            "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.6107 - acc: 0.6692 - val_loss: 0.7166 - val_acc: 0.5669\n",
            "Epoch 3/8\n",
            "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.4935 - acc: 0.7611 - val_loss: 0.3545 - val_acc: 0.8471\n",
            "Epoch 4/8\n",
            "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.3386 - acc: 0.8546 - val_loss: 0.5060 - val_acc: 0.7877\n",
            "Epoch 5/8\n",
            "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.2693 - acc: 0.8900 - val_loss: 0.3715 - val_acc: 0.8476\n",
            "Epoch 6/8\n",
            "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.2071 - acc: 0.9186 - val_loss: 0.2869 - val_acc: 0.8864\n",
            "Epoch 7/8\n",
            "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1613 - acc: 0.9381 - val_loss: 0.2815 - val_acc: 0.8863\n",
            "Epoch 8/8\n",
            "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1232 - acc: 0.9542 - val_loss: 0.3262 - val_acc: 0.8827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7wItjMNNu8i",
        "colab_type": "code",
        "outputId": "6209eadf-b4c8-4628-ddb2-5eb756c7bf16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model_fc = Sequential()\n",
        "\n",
        "model_fc.add(Embedding(max_features, 32, input_shape=(maxlen,)))\n",
        "model_fc.add(Flatten())\n",
        "\n",
        "# model_fc.summary()\n",
        "\n",
        "model_fc.add(Dense(4, activation=\"relu\"))\n",
        "model_fc.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model_fc.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "model_fc.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))\n",
        "\n",
        "model_fc.save(\"fc{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 6s 249us/sample - loss: 0.6933 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 6s 236us/sample - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6931 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 6s 236us/sample - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 6s 236us/sample - loss: 0.6933 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 6s 244us/sample - loss: 0.6395 - acc: 0.6431 - val_loss: 0.5986 - val_acc: 0.7008\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 6s 246us/sample - loss: 0.5438 - acc: 0.7421 - val_loss: 0.5867 - val_acc: 0.6996\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 6s 237us/sample - loss: 0.4815 - acc: 0.7863 - val_loss: 0.6103 - val_acc: 0.6972\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 6s 235us/sample - loss: 0.4192 - acc: 0.8258 - val_loss: 0.6751 - val_acc: 0.6895\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 6s 235us/sample - loss: 0.3615 - acc: 0.8588 - val_loss: 0.7073 - val_acc: 0.6840\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 6s 237us/sample - loss: 0.3078 - acc: 0.8887 - val_loss: 0.7659 - val_acc: 0.6757\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 6s 236us/sample - loss: 0.2597 - acc: 0.9122 - val_loss: 0.8865 - val_acc: 0.6734\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 6s 235us/sample - loss: 0.2206 - acc: 0.9298 - val_loss: 0.9237 - val_acc: 0.6680\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 6s 237us/sample - loss: 0.1887 - acc: 0.9437 - val_loss: 1.0005 - val_acc: 0.6669\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 6s 235us/sample - loss: 0.1610 - acc: 0.9546 - val_loss: 1.1603 - val_acc: 0.6619\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 6s 238us/sample - loss: 0.1417 - acc: 0.9634 - val_loss: 1.2512 - val_acc: 0.6609\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 6s 238us/sample - loss: 0.1249 - acc: 0.9698 - val_loss: 1.3053 - val_acc: 0.6586\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 6s 232us/sample - loss: 0.1141 - acc: 0.9734 - val_loss: 1.4399 - val_acc: 0.6586\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 6s 233us/sample - loss: 0.1041 - acc: 0.9770 - val_loss: 1.4399 - val_acc: 0.6574\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 6s 233us/sample - loss: 0.0988 - acc: 0.9780 - val_loss: 1.4359 - val_acc: 0.6547\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 6s 234us/sample - loss: 0.0998 - acc: 0.9770 - val_loss: 1.5732 - val_acc: 0.6524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRDksIGMVTk4",
        "colab_type": "code",
        "outputId": "c971e78f-5c71-4ab5-d60b-a36088a7435d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "clear_session()\n",
        "\n",
        "model_oh = Sequential()\n",
        "\n",
        "model_oh.add(Flatten())\n",
        "\n",
        "model_oh.add(Dense(32, activation=\"relu\"))\n",
        "model_oh.add(Dense(8, activation=\"relu\"))\n",
        "model_oh.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model_oh.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "# print(x_train.shape, x_test.shape)\n",
        "\n",
        "model_oh.fit(to_categorical(x_train), y_train, epochs=20, validation_data=(to_categorical(x_test), y_test))\n",
        "\n",
        "model_oh.save(\"oh{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 10s 392us/sample - loss: 0.6741 - acc: 0.5706 - val_loss: 0.6670 - val_acc: 0.5871\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 9s 380us/sample - loss: 0.4978 - acc: 0.7636 - val_loss: 0.6482 - val_acc: 0.6514\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 10s 390us/sample - loss: 0.3066 - acc: 0.8698 - val_loss: 0.7889 - val_acc: 0.6456\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 10s 387us/sample - loss: 0.2040 - acc: 0.9178 - val_loss: 1.0261 - val_acc: 0.6422\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 9s 380us/sample - loss: 0.1308 - acc: 0.9504 - val_loss: 1.2673 - val_acc: 0.6342\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0825 - acc: 0.9711 - val_loss: 1.5719 - val_acc: 0.6379\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 9s 377us/sample - loss: 0.0515 - acc: 0.9828 - val_loss: 1.8091 - val_acc: 0.6327\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0293 - acc: 0.9918 - val_loss: 2.1914 - val_acc: 0.6332\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0164 - acc: 0.9959 - val_loss: 2.5106 - val_acc: 0.6300\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 10s 387us/sample - loss: 0.0298 - acc: 0.9906 - val_loss: 2.2463 - val_acc: 0.6261\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 9s 380us/sample - loss: 0.0255 - acc: 0.9914 - val_loss: 2.5391 - val_acc: 0.6266\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0126 - acc: 0.9959 - val_loss: 2.6998 - val_acc: 0.6288\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 9s 377us/sample - loss: 0.0047 - acc: 0.9990 - val_loss: 2.8323 - val_acc: 0.6300\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 9s 374us/sample - loss: 0.0064 - acc: 0.9979 - val_loss: 3.0099 - val_acc: 0.6290\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 9s 378us/sample - loss: 0.0084 - acc: 0.9973 - val_loss: 3.0193 - val_acc: 0.6260\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 9s 377us/sample - loss: 0.0185 - acc: 0.9935 - val_loss: 2.9680 - val_acc: 0.6287\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0169 - acc: 0.9944 - val_loss: 2.9459 - val_acc: 0.6303\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 9s 375us/sample - loss: 0.0085 - acc: 0.9970 - val_loss: 2.7226 - val_acc: 0.6279\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 9s 379us/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 3.0882 - val_acc: 0.6244\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 9s 376us/sample - loss: 6.6567e-04 - acc: 0.9999 - val_loss: 3.2743 - val_acc: 0.6326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW0MY4GeiJB9",
        "colab_type": "code",
        "outputId": "1a8305a3-f092-412e-cfed-c4d3cb222198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
            "\u001b[K     |████████████████████████████████| 852.3MB 46.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hwmet2gs/wheels/0d/bc/67/e6a9108ab86cd076703af19ad4e0f02f57381ac6583df16249\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en_core_web_lg\n",
            "\n",
            "    You can now load the model via spacy.load('en_core_web_lg')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u4s99Kqkp8W",
        "colab_type": "code",
        "outputId": "634144ee-a68d-4f8c-85cf-38282f9ad9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsu2knprxtRM",
        "colab_type": "code",
        "outputId": "f3d35930-dc46-482b-f7cf-258897066b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-30 12:49:43--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.164.117\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.164.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  58.4MB/s    in 23s     \n",
            "\n",
            "2019-05-30 12:50:06 (69.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ5TqoK1y6F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip 'GoogleNews-vectors-negative300.bin.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dBQpjgSwnPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV3--L8Txi6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,max_features)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=max_features:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVvoaiL80V_k",
        "colab_type": "code",
        "outputId": "f17ab45d-6fbf-43c1-baf8-ed33206b1ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfmQkc4a0OHX",
        "colab_type": "code",
        "outputId": "24033df6-88ca-4f2a-a031-be9c0fc3d0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}