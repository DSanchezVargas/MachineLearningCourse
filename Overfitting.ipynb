{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overfitting",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RFajardoMonzon/MachineLearningCourse/blob/master/Overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tDvrLJWNlb5",
        "colab_type": "text"
      },
      "source": [
        "# Notebook 16 - Overfitting.\n",
        "\n",
        "\n",
        "*   Recuerda que puedes consultar la documentación sobre una función escribiendo **?** justo después de la función: *Ejemplo: np.maximum?*\n",
        "*   Puedes ejecutar el contenido de una celda con el atajo de teclado **CTRL+ENTER**\n",
        "*   Utiliza **TAB** cada vez que quieras autocompletar una llamada a una función.\n",
        "*   Puedes ejecutar instrucciones de bash directamente desde el notebook usando **!** : *Ejemplo: !pip install tensorflow*\n",
        "*   Recuerda que Google es tu amigo, y saber buscar la información en las documentaciones de las librerías es muy importante.\n",
        "*   Una solución correcta no es la que funciona sino la que se entiende!\n",
        "*   No dudes en preguntar cualquier duda al profesor que lleva todo el día dando la turra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oo0phTtQP83",
        "colab_type": "text"
      },
      "source": [
        "## 1. ¿Esta opinión es tuya o de otro?\n",
        "\n",
        "\n",
        "Hoy vamos a trabajar  nuevamente con el dataset  de reviews de películas recopilas de la web IMDB. Ayer entrenamos a nuestro clasificador para que a partir de una revie, supiera predecir si el sentimiento asociado a esa opinión era *positiva* o *negativa*. Ayer conseguimos, combinando diferentes arquitecturas, que nuestro clasificador pudiera predecir con una precisión superior al 80% en el set de validación.\n",
        "\n",
        "> > > > > <img src=https://www.samyzaf.com/ML/imdb/review1.png width=500px>\n",
        "\n",
        " Sin embargo, esta cifra rápidamente se deterioraba con la aparición del ***overfitting*** en nuestro modelo. Hoy probaremos diferentes técnicas de ***regularización*** para tratar de solucionar esto.\n",
        " \n",
        " 1. Re-entrena el modelo que diseñaste ayer (*si no lo tienes, comienza trabajando en este*). Entrena durante varias ***epochs*** (e.g. epochs=20) y registra los diferentes valores de *loss* alcanzado por el modelo durante cada *epoch* para el train y test set. Una vez lo tengas, representalos en una misma gráfica y evalua si hay existencia de overfitting. Después, haz uso del EarlyStopping para que tu modelo pare automáticamente en presencia del overfitting.***(Bonus)*** Puedes probar a visualizarlo en tiempo real a través de tensorboard, o configurando tu propio *callback*()\n",
        " \n",
        " 2.  Conjuntamente prueba a entrenar a un modelo con mayor capacidad, y a otros modelo con menor capacidad. Grafícalos conjuntamente de manera que se pueda observar los fenómenos del ***underfitting()*** y el ***overfitting()***. Utiliza corréctamente las leyendas para identificar a cada modelo.\n",
        " \n",
        " 3. Finalmente repite el experimento para diferentes configuraciones de redes, que hagan uso del **Dropout**, **BatchNormalization** y **Weight Decay**, con diferentes valores de sus parámetros. Obtén al menos 9 combinaciones diferentes de entrenamientos y represéntalos conjuntamente en un gráfico. (dada la densidad de ***plots*** que puede haber, en este apartado no representes el ***training_loss***)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVuz5Sj6N3Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "old = np.load\n",
        "\n",
        "np.load = lambda *a,**k: old(*a, allow_pickle=True, **k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_hgkReN5jn",
        "colab_type": "code",
        "outputId": "57d9d77f-3c24-4075-d6b4-e0684aa45297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_features = 10000 # Número máximo de palabras diferentes de nuestro vocabulario.\n",
        "maxlen = 300         # Número máximo de palabras en cada texto.\n",
        "\n",
        "# Carga el texto ya tokenizado.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features) \n",
        "\n",
        "# Preprocesado de los textos para igualar todas sus longitudes.\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) \n",
        "\n",
        "# Utiliza esta línea para obtener la palabra asociada a cada índice.\n",
        "# imdb.get_word_index()\n",
        "nModel = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnBL78vv9nE2",
        "colab_type": "code",
        "outputId": "614fe6b0-6ae4-451e-8624-d2b3c02e419b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(max_features, 128))\n",
        "\n",
        "model.add(CuDNNLSTM(128))\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=SGD(1.5), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))\n",
        "\n",
        "model.save(\"LSTM{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 21s 830us/sample - loss: 0.6849 - acc: 0.5381 - val_loss: 0.6050 - val_acc: 0.6554\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 17s 690us/sample - loss: 0.6075 - acc: 0.6644 - val_loss: 0.5239 - val_acc: 0.7380\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 17s 679us/sample - loss: 0.4497 - acc: 0.7898 - val_loss: 0.3945 - val_acc: 0.8235\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 17s 681us/sample - loss: 0.3132 - acc: 0.8681 - val_loss: 0.2812 - val_acc: 0.8840\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 17s 681us/sample - loss: 0.2373 - acc: 0.9057 - val_loss: 0.3502 - val_acc: 0.8574\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 17s 686us/sample - loss: 0.1815 - acc: 0.9314 - val_loss: 0.2776 - val_acc: 0.8871\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 17s 693us/sample - loss: 0.1434 - acc: 0.9468 - val_loss: 0.2977 - val_acc: 0.8861\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 17s 689us/sample - loss: 0.1064 - acc: 0.9626 - val_loss: 0.3748 - val_acc: 0.8745\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 17s 686us/sample - loss: 0.0830 - acc: 0.9720 - val_loss: 0.4201 - val_acc: 0.8743\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 17s 685us/sample - loss: 0.0635 - acc: 0.9793 - val_loss: 0.5375 - val_acc: 0.8667\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 17s 688us/sample - loss: 0.0506 - acc: 0.9835 - val_loss: 0.5163 - val_acc: 0.8686\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 17s 683us/sample - loss: 0.0431 - acc: 0.9863 - val_loss: 0.5412 - val_acc: 0.8732\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 17s 685us/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.6307 - val_acc: 0.8691\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 17s 685us/sample - loss: 0.0259 - acc: 0.9920 - val_loss: 0.6339 - val_acc: 0.8679\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 17s 695us/sample - loss: 0.0219 - acc: 0.9932 - val_loss: 0.7197 - val_acc: 0.8569\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 17s 684us/sample - loss: 0.0293 - acc: 0.9903 - val_loss: 0.7267 - val_acc: 0.8662\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 17s 678us/sample - loss: 0.0218 - acc: 0.9924 - val_loss: 0.7775 - val_acc: 0.8724\n",
            "Epoch 18/20\n",
            "11392/25000 [============>.................] - ETA: 6s - loss: 0.0094 - acc: 0.9972"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2e4xVbX_4ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = load_model(\"LSTM1.h5\")\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = history.epoch\n",
        "\n",
        "plt.plot(epochs, loss, label=\"loss\")\n",
        "plt.plot(epochs, val_loss, label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFtXviKICWcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "callback = [EarlyStopping(\"val_loss\", patience=5)]\n",
        "\n",
        "model_ES = Sequential()\n",
        "\n",
        "model_ES.add(Embedding(max_features, 128))\n",
        "\n",
        "model_ES.add(CuDNNLSTM(128))\n",
        "\n",
        "model_ES.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model_ES.compile(optimizer=SGD(1.5), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "history_ES = model_ES.fit(x_train, y_train, epochs=20,\n",
        "                       validation_data=(x_test, y_test), callbacks=callback)\n",
        "\n",
        "model_ES.save(\"LSTM_ES{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBPXPFlbDyXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_ES = history_ES.history[\"loss\"]\n",
        "val_loss_ES = history_ES.history[\"val_loss\"]\n",
        "epochs_ES = history_ES.epoch\n",
        "\n",
        "plt.plot(epochs_ES, loss_ES, label=\"loss\")\n",
        "plt.plot(epochs_ES, val_loss_ES, label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYGhI4aTFOpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_uf = Sequential()\n",
        "\n",
        "\n",
        "model_uf.add(Embedding(max_features, 4, input_shape=(maxlen,), trainable=False))\n",
        "\n",
        "# model_fc.summary()\n",
        "\n",
        "model_uf.add(CuDNNLSTM(4))\n",
        "\n",
        "model_uf.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model_uf.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "history_uf = model_uf.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))\n",
        "\n",
        "model_uf.save(\"LSTM_uf{}.h5\".format(nModel))\n",
        "\n",
        "nModel += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9rPNcXmTYUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mini_x_train = x_train[:1000]\n",
        "mini_y_train = y_train[:1000]\n",
        "\n",
        "mini_x_test = x_test[:1000]\n",
        "mini_y_test = y_test[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC1gnPwtQ5jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "history = []\n",
        "\n",
        "for i in np.arange(15):\n",
        "\n",
        "  clear_session()\n",
        "  \n",
        "  title = \"\"\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(max_features, 32))\n",
        "\n",
        "  model.add(CuDNNLSTM(64))\n",
        "\n",
        "  if i >= 5: \n",
        "    title += \"Dropout 1 = {}\".format(i%5 / 10)\n",
        "    model.add(Dropout(i%5 / 10))\n",
        "\n",
        "  model.add(Dense(64))\n",
        "\n",
        "#   model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Activation(\"relu\"))\n",
        "\n",
        "  if i < 5 or i >= 10:\n",
        "    title += \"Dropout 2 = {}\".format(i%5 /10)\n",
        "    model.add(Dropout(i%5 / 10))\n",
        "\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "  hist = model.fit(mini_x_train, mini_y_train, epochs=20, \n",
        "                           validation_data=(mini_x_test, mini_y_test))\n",
        "\n",
        "  model.save(\"LSTM_DO{}.h5\".format(nModel))\n",
        "\n",
        "  nModel += 1\n",
        "  \n",
        "  val_loss = hist.history[\"val_loss\"]\n",
        "  epochs = hist.epoch\n",
        "  history.append(hist)\n",
        "\n",
        "  plt.plot(epochs, val_loss, label=title)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}